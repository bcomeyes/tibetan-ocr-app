{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51139145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\nPyBo (Tibetan Tokenizer) - Deep Dive Exploration\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\nWHAT IS PYBO?\\nPyBo is a rule-based word tokenizer for Tibetan text. It was developed by the \\nEsukhia project with funding from the Khyentse Foundation ($22,000 to kickstart)\\nand BDRC (2 staff for 6 months for data curation).\\n\\nHOW IT WORKS - THE CORE ALGORITHM:\\n1. Text Preprocessing: Breaks input into chunks (syllables, punctuation, non-Tibetan)\\n2. Syllable Cleaning: Removes punctuation and spaces from syllable chunks  \\n3. Trie Matching: Walks through a Trie data structure built from the THL lexicon\\n4. Longest Match: Finds the longest possible word match in the lexicon\\n5. POS Tagging: Assigns part-of-speech tags based on lexicon entries\\n\\nTHE TRIE DATA STRUCTURE:\\nA trie (from \"retrieval\") is like a tree where each node represents a character.\\nIt allows extremely fast lookup of words. For example:\\n\\nRoot\\nâ”œâ”€ à½–\\nâ”‚  â”œâ”€ à½‘ (complete word: \"à½–à½‘\")\\nâ”‚  â””â”€ à½‘à½º (complete word: \"à½–à½‘à½º\")\\nâ””â”€ à½‚\\n   â””â”€ à½“à¼‹à½¦ (complete word: \"à½‚à½“à½¦\")\\n\\nThis structure lets PyBo quickly check if \"à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦\" contains valid words by\\nwalking down the tree character by character.\\n\\nWHAT MAKES IT \"RULE-BASED\"?\\n- Uses manually curated THL (Tibetan & Himalayan Library) lexicon\\n- Applies handcrafted heuristics for syllable boundary detection\\n- Follows explicit rules for affix handling (e.g., à½¢à¼‹, à½¦à¼‹, à½ à½²à¼‹)\\n- No machine learning - pure linguistic rules\\n\\nSTRENGTHS:\\nâœ“ Excellent for classical Tibetan (scriptures, formal texts)\\nâœ“ High accuracy on well-structured text\\nâœ“ Interpretable - you can see WHY it made a decision\\nâœ“ Provides POS tags (noun, verb, particle, etc.)\\nâœ“ Shows syllable structure information\\n\\nWEAKNESSES:\\nâœ— Limited to words in the THL lexicon (fixed vocabulary)\\nâœ— Struggles with modern Tibetan vocabulary\\nâœ— Can\\'t adapt to new words without manual lexicon updates\\nâœ— May fail on informal or domain-shifted text\\n\\nUSE CASE FOR OCR QUALITY SCORING:\\nFor our grid search, PyBo could help identify \"valid words\" vs OCR garbage by\\nchecking if detected text exists in the THL lexicon. If OCR output produces\\nmostly non-lexicon words, we know it\\'s probably garbage.\\n\\nAuthor: Matt\\nDate: 2026-01-25\\nPurpose: Explore PyBo\\'s capabilities for potential use in OCR quality scoring\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PyBo (Tibetan Tokenizer) - Deep Dive Exploration\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "WHAT IS PYBO?\n",
    "PyBo is a rule-based word tokenizer for Tibetan text. It was developed by the \n",
    "Esukhia project with funding from the Khyentse Foundation ($22,000 to kickstart)\n",
    "and BDRC (2 staff for 6 months for data curation).\n",
    "\n",
    "HOW IT WORKS - THE CORE ALGORITHM:\n",
    "1. Text Preprocessing: Breaks input into chunks (syllables, punctuation, non-Tibetan)\n",
    "2. Syllable Cleaning: Removes punctuation and spaces from syllable chunks  \n",
    "3. Trie Matching: Walks through a Trie data structure built from the THL lexicon\n",
    "4. Longest Match: Finds the longest possible word match in the lexicon\n",
    "5. POS Tagging: Assigns part-of-speech tags based on lexicon entries\n",
    "\n",
    "THE TRIE DATA STRUCTURE:\n",
    "A trie (from \"retrieval\") is like a tree where each node represents a character.\n",
    "It allows extremely fast lookup of words. For example:\n",
    "\n",
    "Root\n",
    "â”œâ”€ à½–\n",
    "â”‚  â”œâ”€ à½‘ (complete word: \"à½–à½‘\")\n",
    "â”‚  â””â”€ à½‘à½º (complete word: \"à½–à½‘à½º\")\n",
    "â””â”€ à½‚\n",
    "   â””â”€ à½“à¼‹à½¦ (complete word: \"à½‚à½“à½¦\")\n",
    "\n",
    "This structure lets PyBo quickly check if \"à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦\" contains valid words by\n",
    "walking down the tree character by character.\n",
    "\n",
    "WHAT MAKES IT \"RULE-BASED\"?\n",
    "- Uses manually curated THL (Tibetan & Himalayan Library) lexicon\n",
    "- Applies handcrafted heuristics for syllable boundary detection\n",
    "- Follows explicit rules for affix handling (e.g., à½¢à¼‹, à½¦à¼‹, à½ à½²à¼‹)\n",
    "- No machine learning - pure linguistic rules\n",
    "\n",
    "STRENGTHS:\n",
    "âœ“ Excellent for classical Tibetan (scriptures, formal texts)\n",
    "âœ“ High accuracy on well-structured text\n",
    "âœ“ Interpretable - you can see WHY it made a decision\n",
    "âœ“ Provides POS tags (noun, verb, particle, etc.)\n",
    "âœ“ Shows syllable structure information\n",
    "\n",
    "WEAKNESSES:\n",
    "âœ— Limited to words in the THL lexicon (fixed vocabulary)\n",
    "âœ— Struggles with modern Tibetan vocabulary\n",
    "âœ— Can't adapt to new words without manual lexicon updates\n",
    "âœ— May fail on informal or domain-shifted text\n",
    "\n",
    "USE CASE FOR OCR QUALITY SCORING:\n",
    "For our grid search, PyBo could help identify \"valid words\" vs OCR garbage by\n",
    "checking if detected text exists in the THL lexicon. If OCR output produces\n",
    "mostly non-lexicon words, we know it's probably garbage.\n",
    "\n",
    "Author: Matt\n",
    "Date: 2026-01-25\n",
    "Purpose: Explore PyBo's capabilities for potential use in OCR quality scoring\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d90d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSTALLATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Run this in your terminal (not in the notebook):\n",
    "# pip install pybo --break-system-packages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "920f089e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36e2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Using file: /home/matt/Documents/tibetan-ocr-app/input_files/toh_4434_T.json\n",
      "ðŸ“š Loaded 3 folios from Tengyur\n",
      "ðŸ“Š Total characters: 4,624\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LOAD TEST DATA FROM TENGYUR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def load_tengyur_sample(json_path: str, num_folios: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load clean Tibetan text from Tengyur JSON file.\n",
    "    \n",
    "    This gives us known-good text to test PyBo's capabilities.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to Tengyur JSON file\n",
    "        num_folios: Number of folios to load\n",
    "        \n",
    "    Returns:\n",
    "        List of Tibetan text strings\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = [folio['content'] for folio in data[:num_folios]]\n",
    "    \n",
    "    total_chars = sum(len(t) for t in texts)\n",
    "    print(f\"ðŸ“š Loaded {num_folios} folios from Tengyur\")\n",
    "    print(f\"ðŸ“Š Total characters: {total_chars:,}\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Load the test data\n",
    "# Point to the input_files directory in your tibetan-ocr-app project\n",
    "from pathlib import Path\n",
    "input_dir = Path.home() / \"Documents\" / \"tibetan-ocr-app\" / \"input_files\"\n",
    "\n",
    "# Look for JSON files in that directory\n",
    "json_files = list(input_dir.glob(\"*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "    print(f\"âŒ No JSON files found in {input_dir}\")\n",
    "    print(\"   Please add Tengyur JSON files to that directory\")\n",
    "    exit()\n",
    "\n",
    "# Use the first JSON file found\n",
    "tengyur_file = str(json_files[0])\n",
    "print(f\"ðŸ“ Using file: {tengyur_file}\")\n",
    "\n",
    "test_texts = load_tengyur_sample(tengyur_file, num_folios=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ee5901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyBo imported successfully\n",
      "\n",
      "======================================================================\n",
      "INITIALIZING PYBO TOKENIZER\n",
      "======================================================================\n",
      "âœ“ Tokenizer initialized\n",
      "  This means PyBo will:\n",
      "    - Tokenize text into words\n",
      "    - Use the THL lexicon for matching\n",
      "    - Provide POS tags where available\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INITIALIZE PYBO TOKENIZER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "try:\n",
    "    from pybo import WordTokenizer\n",
    "    \n",
    "    print(\"âœ… PyBo imported successfully\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INITIALIZING PYBO TOKENIZER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize with default configuration\n",
    "    # This loads the THL lexicon and builds the Trie data structure\n",
    "    tokenizer = WordTokenizer()\n",
    "    \n",
    "    print(\"âœ“ Tokenizer initialized\")\n",
    "    print(\"  This means PyBo will:\")\n",
    "    print(\"    - Tokenize text into words\")\n",
    "    print(\"    - Use the THL lexicon for matching\")\n",
    "    print(\"    - Provide POS tags where available\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PyBo not installed: {e}\")\n",
    "    print(\"   Install with: pip install pybo --break-system-packages\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2906ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: BASIC TOKENIZATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Input text (first 200 chars):\n",
      "   à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢à¼‹à½¦à¾à½‘à¼‹à½‘à½´à¼ à½¦à½²à½‘à¾¡à¾·à¼‹à½¦à¼‹à½¢à¼ à½–à½¼à½‘à¼‹à½¦à¾à½‘à¼‹à½‘à½´à¼ à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼ à½–à½˜à¼‹à½”à½¼à¼‹à½‘à½„à¼‹à½”à½¼à½ à½¼à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½€à¾±à½²à¼‹à½‚à½žà½´à½„à¼‹à½ à½‘à½²à¼‹à½£à¼‹à½£à½ºà½ à½´à¼‹à½–à½¦à¾¡à½´à½¦à¼‹à½”à¼‹à½¦à½´\n",
      "\n",
      "ðŸ” PyBo found 40 tokens\n",
      "\n",
      "ðŸ’¡ First 10 tokens:\n",
      "   1. 'à¼„à¼ à¼' (type: PUNCT, POS: )\n",
      "   2. 'à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹' (type: TEXT, POS: NOUN)\n",
      "   3. 'à½‚à½…à½ºà½¦à¼‹à½”à¼‹' (type: TEXT, POS: VERB)\n",
      "   4. 'à½‚à¾²à½´à½–à¼‹à½”à¼‹' (type: TEXT, POS: VERB)\n",
      "   5. 'à½žà½ºà½¦à¼‹' (type: TEXT, POS: PART)\n",
      "   6. 'à½–à¾±à¼‹à½–à¼‹' (type: TEXT, POS: VERB)\n",
      "   7. 'à½£à½ºà½ à½´à¼‹' (type: TEXT, POS: NOUN)\n",
      "   8. 'à½¦à½´à½˜à¼‹à½…à½´à¼‹' (type: TEXT, POS: OTHER)\n",
      "   9. 'à½¢à¾©à¼‹' (type: TEXT, POS: NUM)\n",
      "   10. 'à½‚à½…à½²à½‚à¼‹à½”à¼‹' (type: TEXT, POS: ADJ)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 1: BASIC TOKENIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: BASIC TOKENIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Take a small sample\n",
    "sample_text = test_texts[0][:200]\n",
    "print(f\"\\nðŸ“ Input text (first 200 chars):\")\n",
    "print(f\"   {sample_text}\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(f\"\\nðŸ” PyBo found {len(tokens)} tokens\")\n",
    "print(f\"\\nðŸ’¡ First 10 tokens:\")\n",
    "for i, token in enumerate(tokens[:10]):\n",
    "    # Get attributes that actually exist\n",
    "    token_text = token.text if hasattr(token, 'text') else str(token)\n",
    "    token_pos = token.pos if hasattr(token, 'pos') else 'N/A'\n",
    "    token_type = token.chunk_type if hasattr(token, 'chunk_type') else 'N/A'\n",
    "    print(f\"   {i+1}. '{token_text}' (type: {token_type}, POS: {token_pos})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9f312e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: UNDERSTANDING TOKEN ATTRIBUTES\n",
      "======================================================================\n",
      "\n",
      "Each PyBo token has multiple attributes:\n",
      "Let's examine one token in detail:\n",
      "\n",
      "Token: 'à¼„à¼ à¼'\n",
      "\n",
      "Available attributes: affix, affix_host, affixation, char_types, chunk_type, form_freq, freq, has_merged_dagdra, lemma, len...\n",
      "  .text        = 'à¼„à¼ à¼'      (the actual text)\n",
      "  .pos         = ''       (part of speech)\n",
      "  .chunk_type  = 'PUNCT'(token type)\n",
      "  .len         = 4         (length)\n",
      "\n",
      "ðŸ’¡ Key insight: These attributes tell us if this is a KNOWN word in the lexicon\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 2: UNDERSTANDING TOKEN ATTRIBUTES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: UNDERSTANDING TOKEN ATTRIBUTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nEach PyBo token has multiple attributes:\")\n",
    "print(\"Let's examine one token in detail:\\n\")\n",
    "\n",
    "# Pick the first token to examine\n",
    "if tokens:\n",
    "    example_token = tokens[0]\n",
    "    \n",
    "    print(f\"Token: '{example_token.text if hasattr(example_token, 'text') else example_token}'\")\n",
    "    \n",
    "    # List all available attributes\n",
    "    attrs = [attr for attr in dir(example_token) if not attr.startswith('_')]\n",
    "    print(f\"\\nAvailable attributes: {', '.join(attrs[:10])}...\")\n",
    "    \n",
    "    # Show the most useful ones\n",
    "    if hasattr(example_token, 'text'):\n",
    "        print(f\"  .text        = '{example_token.text}'      (the actual text)\")\n",
    "    if hasattr(example_token, 'pos'):\n",
    "        print(f\"  .pos         = '{example_token.pos}'       (part of speech)\")\n",
    "    if hasattr(example_token, 'chunk_type'):\n",
    "        print(f\"  .chunk_type  = '{example_token.chunk_type}'(token type)\")\n",
    "    if hasattr(example_token, 'tag'):\n",
    "        print(f\"  .tag         = '{example_token.tag}'       (grammatical tag)\")\n",
    "    if hasattr(example_token, 'len'):\n",
    "        print(f\"  .len         = {example_token.len}         (length)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Key insight: These attributes tell us if this is a KNOWN word in the lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "261b6ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: EXTRACTING WORDS BY PART OF SPEECH\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Part-of-Speech distribution:\n",
      "   NOUN           :  27 tokens\n",
      "   PART           :  22 tokens\n",
      "   NO_POS         :  20 tokens\n",
      "   VERB           :  11 tokens\n",
      "   OTHER          :   4 tokens\n",
      "   NUM            :   3 tokens\n",
      "   ADJ            :   2 tokens\n",
      "   PROPN          :   2 tokens\n",
      "   NON_WORD       :   1 tokens\n",
      "   DET            :   1 tokens\n",
      "\n",
      "ðŸ“Œ Sample nouns: à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹ à½£à½ºà½ à½´à¼‹ à½¦à¾à½‘à¼‹ à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹ à½–à½˜à¼‹à½”à½¼à¼‹\n",
      "âš¡ Sample verbs: à½‚à½…à½ºà½¦à¼‹à½”à¼‹ à½‚à¾²à½´à½–à¼‹à½”à¼‹ à½–à¾±à¼‹à½–à¼‹ à½–à½žà½´à½‚à½¦ à½‚à½…à½ºà½¦à¼‹à½”à¼‹\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 3: EXTRACTING WORDS BY PART OF SPEECH\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: EXTRACTING WORDS BY PART OF SPEECH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tokenize a longer sample\n",
    "longer_sample = test_texts[0][:500]\n",
    "tokens = tokenizer.tokenize(longer_sample)\n",
    "\n",
    "# Count different POS tags\n",
    "pos_counts = Counter(t.pos for t in tokens if hasattr(t, 'pos') and t.pos)\n",
    "\n",
    "print(\"\\nðŸ“Š Part-of-Speech distribution:\")\n",
    "for pos, count in pos_counts.most_common():\n",
    "    print(f\"   {pos:15s}: {count:3d} tokens\")\n",
    "\n",
    "# Extract specific POS types\n",
    "nouns = [t.text for t in tokens if hasattr(t, 'pos') and t.pos and 'NOUN' in str(t.pos)]\n",
    "verbs = [t.text for t in tokens if hasattr(t, 'pos') and t.pos and 'VERB' in str(t.pos)]\n",
    "\n",
    "if nouns:\n",
    "    print(f\"\\nðŸ“Œ Sample nouns: {' '.join(nouns[:5])}\")\n",
    "if verbs:\n",
    "    print(f\"âš¡ Sample verbs: {' '.join(verbs[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7991410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: OCR QUALITY SCORING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Scoring known-good Tengyur text:\n",
      "   Total syllables:    323\n",
      "   Valid words:        321\n",
      "   Unknown words:      2\n",
      "   âœ¨ SCORE:           99.38%\n",
      "   â±ï¸  Processing time:  0.3279s\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 4: CALCULATING \"VALID WORD PERCENTAGE\" (OCR QUALITY SCORE)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 4: OCR QUALITY SCORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_pybo_score(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate what percentage of text consists of valid Tibetan words.\n",
    "    \n",
    "    This is the KEY function for OCR quality scoring.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with scoring metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    valid_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Get attributes\n",
    "        chunk_type = getattr(token, 'chunk_type', None)\n",
    "        pos = getattr(token, 'pos', None)\n",
    "        \n",
    "        # Skip ONLY punctuation\n",
    "        if chunk_type == 'PUNCT' or pos == '':\n",
    "            continue\n",
    "            \n",
    "        total_tokens += 1\n",
    "        \n",
    "        # A token is \"valid\" if it has a real POS tag (not NON_WORD, not empty)\n",
    "        if pos and pos not in ['NON_WORD', 'non-word', '', None]:\n",
    "            valid_tokens += 1\n",
    "        else:\n",
    "            unknown_tokens += 1\n",
    "    \n",
    "    valid_percentage = (valid_tokens / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'total_syllables': total_tokens,\n",
    "        'valid_words': valid_tokens,\n",
    "        'unknown_words': unknown_tokens,\n",
    "        'valid_percentage': valid_percentage,\n",
    "        'processing_time': elapsed\n",
    "    }\n",
    "\n",
    "# Test on known-good text\n",
    "good_score = calculate_pybo_score(test_texts[0])\n",
    "\n",
    "print(\"\\nðŸ“Š Scoring known-good Tengyur text:\")\n",
    "print(f\"   Total syllables:    {good_score['total_syllables']}\")\n",
    "print(f\"   Valid words:        {good_score['valid_words']}\")\n",
    "print(f\"   Unknown words:      {good_score['unknown_words']}\")\n",
    "print(f\"   âœ¨ SCORE:           {good_score['valid_percentage']:.2f}%\")\n",
    "print(f\"   â±ï¸  Processing time:  {good_score['processing_time']:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e0cdbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing different OCR quality levels:\n",
      "\n",
      "DIAGNOSTIC CHECK:\n",
      "Original: à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢\n",
      "Corrupted: à½…à¼à½–à½‘à½¦à¼‹à½–à¼‹à½à½‡à¼à½‘à¼‹à½‚à½…à½ºà¼à¼‹à½˜à½“à½‚à½–à½´à½–à¼‹à½”à¼‹à½…à½ºà½€à½€à½–à¼‹à¼‹à½€à¼‹à½£à½ºà½€à¼à¼‹à¼à½”à½˜à½‡à½…à½„à¼‹à½„à¾©à½„à½‘à½…à½²à½‚à½à½”à¼‹à½˜à¼‹à½€à½„à½¦à½”à¼ à¼à½€à¼„à¼…à¼…à¼à½„à½€à½¢à¾’à½‘à¼‹à½‚à½¢\n",
      "\n",
      "First 5 original tokens with POS:\n",
      "  'à¼„à¼ à¼' â†’ POS: \n",
      "  'à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹' â†’ POS: NOUN\n",
      "  'à½‚à½…à½ºà½¦à¼‹à½”à¼‹' â†’ POS: VERB\n",
      "  'à½‚à¾²à½´à½–à¼‹à½”à¼‹' â†’ POS: VERB\n",
      "  'à½žà½ºà½¦à¼‹' â†’ POS: PART\n",
      "\n",
      "First 5 corrupted tokens with POS:\n",
      "  'à½…' â†’ POS: OTHER\n",
      "  'à¼' â†’ POS: \n",
      "  'à½–à½‘à½¦à¼‹à½–à¼‹' â†’ POS: NO_POS\n",
      "  'à½à½‡' â†’ POS: NON_WORD\n",
      "  'à¼' â†’ POS: \n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 5: TESTING ON CORRUPTED TEXT (SIMULATED BAD OCR)\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª Testing different OCR quality levels:\n",
      "\n",
      "Original (perfect OCR)         â†’ Score:  98.92%\n",
      "Slightly corrupted (20%)       â†’ Score:  54.46%\n",
      "Heavily corrupted (50%)        â†’ Score:  26.83%\n",
      "\n",
      "ðŸ’¡ Insight: PyBo can distinguish good OCR from garbage!\n",
      "   We can use this to automatically rank OCR outputs\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 5: SIMULATING OCR ERRORS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nðŸ§ª Testing different OCR quality levels:\\n\")\n",
    "\n",
    "# DIAGNOSTIC: Let's manually check a few tokens\n",
    "sample = test_texts[0][:500]\n",
    "corrupted = corrupt_text(sample, 0.5)\n",
    "\n",
    "print(\"DIAGNOSTIC CHECK:\")\n",
    "print(f\"Original: {sample[:80]}\")\n",
    "print(f\"Corrupted: {corrupted[:80]}\")\n",
    "\n",
    "tokens_orig = tokenizer.tokenize(sample)\n",
    "tokens_corrupt = tokenizer.tokenize(corrupted)\n",
    "\n",
    "print(f\"\\nFirst 5 original tokens with POS:\")\n",
    "for t in tokens_orig[:5]:\n",
    "    print(f\"  '{t.text}' â†’ POS: {getattr(t, 'pos', 'NONE')}\")\n",
    "\n",
    "print(f\"\\nFirst 5 corrupted tokens with POS:\")\n",
    "for t in tokens_corrupt[:5]:\n",
    "    print(f\"  '{t.text}' â†’ POS: {getattr(t, 'pos', 'NONE')}\")\n",
    "print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 5: TESTING ON CORRUPTED TEXT (SIMULATED BAD OCR)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def corrupt_text(text: str, corruption_rate: float = 0.2) -> str:\n",
    "    \"\"\"\n",
    "    Randomly corrupt text to simulate OCR errors.\n",
    "    \n",
    "    Args:\n",
    "        text: Clean Tibetan text\n",
    "        corruption_rate: Fraction of characters to corrupt (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Corrupted text\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    chars = list(text)\n",
    "    num_to_corrupt = int(len(chars) * corruption_rate)\n",
    "    \n",
    "    # Common Tibetan consonants to use as random replacements\n",
    "    garbage = ['à½€', 'à½‚', 'à½„', 'à½…', 'à½‡', 'à½', 'à½‘', 'à½“', 'à½”', 'à½–', 'à½˜', 'à¼‹', 'à¼']\n",
    "    \n",
    "    positions = random.sample(range(len(chars)), min(num_to_corrupt, len(chars)))\n",
    "    \n",
    "    for pos in positions:\n",
    "        chars[pos] = random.choice(garbage)\n",
    "    \n",
    "    return ''.join(chars)\n",
    "\n",
    "# Test with different corruption levels\n",
    "sample = test_texts[0][:500]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing different OCR quality levels:\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"Original (perfect OCR)\", sample, 0.0),\n",
    "    (\"Slightly corrupted (20%)\", corrupt_text(sample, 0.2), 0.2),\n",
    "    (\"Heavily corrupted (50%)\", corrupt_text(sample, 0.5), 0.5),\n",
    "]\n",
    "\n",
    "for label, text, rate in test_cases:\n",
    "    score = calculate_pybo_score(text)\n",
    "    print(f\"{label:30s} â†’ Score: {score['valid_percentage']:6.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Insight: PyBo can distinguish good OCR from garbage!\")\n",
    "print(\"   We can use this to automatically rank OCR outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "670e230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 6: PERFORMANCE BENCHMARKING\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  How fast can PyBo score OCR outputs?\n",
      "   (Important for processing 1,728 outputs per image!)\n",
      "\n",
      "     100 chars â†’  20.37ms (avg of 5 runs)\n",
      "     500 chars â†’  82.76ms (avg of 5 runs)\n",
      "    1000 chars â†’ 116.07ms (avg of 5 runs)\n",
      "    2000 chars â†’ 196.55ms (avg of 5 runs)\n",
      "\n",
      "ðŸ’¡ For 1,728 outputs:\n",
      "   Estimated scoring time: 339.64s (5.66 minutes)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 6: PERFORMANCE BENCHMARKING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 6: PERFORMANCE BENCHMARKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâ±ï¸  How fast can PyBo score OCR outputs?\")\n",
    "print(\"   (Important for processing 1,728 outputs per image!)\\n\")\n",
    "\n",
    "# Test on different text sizes\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "for size in sizes:\n",
    "    text_sample = test_texts[0][:size]\n",
    "    \n",
    "    # Time multiple runs\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.time()\n",
    "        tokenizer.tokenize(text_sample)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    \n",
    "    print(f\"   {size:5d} chars â†’ {avg_time*1000:6.2f}ms (avg of 5 runs)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ For 1,728 outputs:\")\n",
    "estimated_time = 1728 * (sum(times) / len(times))\n",
    "print(f\"   Estimated scoring time: {estimated_time:.2f}s ({estimated_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f311ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 7: TESTING EDGE CASES\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª How does PyBo handle unusual inputs?\n",
      "\n",
      "   Mostly punctuation        â†’   0.00% valid\n",
      "   Mixed with numbers        â†’ 100.00% valid\n",
      "   Sanskrit mantra           â†’  50.00% valid\n",
      "   Very short text           â†’ 100.00% valid\n",
      "   Empty string              â†’ (empty, skipped)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 7: EDGE CASES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 7: TESTING EDGE CASES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "edge_cases = {\n",
    "    'Mostly punctuation': 'à¼à¼ à¼à¼à¼ à¼à¼à¼ à¼à¼',\n",
    "    'Mixed with numbers': 'à¼¡à¼¢à¼£ à½–à½¼à½‘à¼‹à½¦à¾à½‘à¼‹ à¼¤à¼¥à¼¦',\n",
    "    'Sanskrit mantra': 'à½¨à½¼à½¾à¼‹à½˜à¼‹à½Žà½²à¼‹à½”à½‘à¾¨à½ºà¼‹à½§à½±à½´à¾ƒà¼‹',\n",
    "    'Very short text': 'à½–à½¼à½‘à¼‹',\n",
    "    'Empty string': '',\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ§ª How does PyBo handle unusual inputs?\\n\")\n",
    "\n",
    "for label, text in edge_cases.items():\n",
    "    if text:\n",
    "        score = calculate_pybo_score(text)\n",
    "        print(f\"   {label:25s} â†’ {score['valid_percentage']:6.2f}% valid\")\n",
    "    else:\n",
    "        print(f\"   {label:25s} â†’ (empty, skipped)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10b823d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“‹ SUMMARY: PYBO FOR OCR QUALITY SCORING\n",
      "======================================================================\n",
      "\n",
      "WHAT WE LEARNED:\n",
      "\n",
      "1. PyBo is RULE-BASED - uses THL lexicon for word matching\n",
      "2. Provides POS TAGS - helps identify valid vs invalid words\n",
      "3. FAST ENOUGH - can score 1,728 outputs in reasonable time\n",
      "4. WORKS WELL on classical/formal Tibetan (like our Tengyur test data)\n",
      "5. CAN DISTINGUISH good OCR from garbage via \"valid word percentage\"\n",
      "\n",
      "STRENGTHS FOR OUR USE CASE:\n",
      "âœ“ High accuracy on classical texts (our pecha corpus)\n",
      "âœ“ Clear distinction between lexicon words and garbage\n",
      "âœ“ Fast processing (suitable for batch OCR scoring)\n",
      "âœ“ Provides interpretable results (POS tags)\n",
      "\n",
      "POTENTIAL WEAKNESSES:\n",
      "âœ— Limited to THL lexicon (may miss valid but unlisted words)\n",
      "âœ— Religious/classical vocabulary focus (good for us!)\n",
      "âœ— Can't adapt to new words without manual lexicon updates\n",
      "\n",
      "RECOMMENDED SCORING FUNCTION:\n",
      "```python\n",
      "def score_ocr_with_pybo(text: str) -> float:\n",
      "    tokens = tokenizer.tokenize(text)\n",
      "    \n",
      "    syllables = [t for t in tokens if t.type == 'syl']\n",
      "    valid = [t for t in syllables if t.pos and t.pos != 'non-word']\n",
      "    \n",
      "    return len(valid) / len(syllables) if syllables else 0.0\n",
      "```\n",
      "\n",
      "NEXT STEPS:\n",
      "Compare with Botok and TibetanRuleSeg to see which gives best results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SUMMARY & RECOMMENDATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‹ SUMMARY: PYBO FOR OCR QUALITY SCORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "\n",
    "1. PyBo is RULE-BASED - uses THL lexicon for word matching\n",
    "2. Provides POS TAGS - helps identify valid vs invalid words\n",
    "3. FAST ENOUGH - can score 1,728 outputs in reasonable time\n",
    "4. WORKS WELL on classical/formal Tibetan (like our Tengyur test data)\n",
    "5. CAN DISTINGUISH good OCR from garbage via \"valid word percentage\"\n",
    "\n",
    "STRENGTHS FOR OUR USE CASE:\n",
    "âœ“ High accuracy on classical texts (our pecha corpus)\n",
    "âœ“ Clear distinction between lexicon words and garbage\n",
    "âœ“ Fast processing (suitable for batch OCR scoring)\n",
    "âœ“ Provides interpretable results (POS tags)\n",
    "\n",
    "POTENTIAL WEAKNESSES:\n",
    "âœ— Limited to THL lexicon (may miss valid but unlisted words)\n",
    "âœ— Religious/classical vocabulary focus (good for us!)\n",
    "âœ— Can't adapt to new words without manual lexicon updates\n",
    "\n",
    "RECOMMENDED SCORING FUNCTION:\n",
    "```python\n",
    "def score_ocr_with_pybo(text: str) -> float:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    syllables = [t for t in tokens if t.type == 'syl']\n",
    "    valid = [t for t in syllables if t.pos and t.pos != 'non-word']\n",
    "    \n",
    "    return len(valid) / len(syllables) if syllables else 0.0\n",
    "```\n",
    "\n",
    "NEXT STEPS:\n",
    "Compare with Botok and TibetanRuleSeg to see which gives best results!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
