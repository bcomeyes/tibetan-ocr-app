{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f5684d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\nBotok (à½–à½¼à½‘à¼‹à½à½¼à½‚) - Tibetan Word Tokenizer - Deep Dive Exploration\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\\nWHAT IS BOTOK?\\nBotok (pronounced [pÊ°Ã¸tÉ”kÌš]) is a state-of-the-art word tokenizer for Tibetan.\\nIt\\'s more advanced than PyBo and provides multiple tokenization modes, POS tagging,\\nlemmatization, and support for custom dialects. Developed by OpenPecha.\\n\\nHOW IT WORKS - THE CORE ALGORITHM:\\n1. Dictionary-Based Matching: Uses a Trie data structure built from multiple lexicons\\n2. Hierarchical Rule Templates: Applies layered grammar rules for normalization\\n3. Affix Handling: Can split or keep affixed particles (à½¢à¼‹, à½¦à¼‹, à½ à½²à¼‹, etc.)\\n4. Lemmatization: Reduces words to their base forms\\n5. Multi-Mode: Offers word, chunk, and space-based tokenization\\n\\nTHE TRIE DATA STRUCTURE (ENHANCED):\\nLike PyBo, Botok uses a trie, but it\\'s MORE sophisticated:\\n- Supports multiple dictionaries simultaneously\\n- Includes frequency information\\n- Handles morphological variations\\n- Allows custom dialect additions\\n\\nRoot\\nâ”œâ”€ à½–\\nâ”‚  â”œâ”€ à½‘ [freq:100, lemma:à½–à½‘]\\nâ”‚  â””â”€ à½‘à½º [freq:500, lemma:à½–à½‘à½º, POS:ADJ]\\nâ”‚       â””â”€ à¼‹à½£à½ºà½‚à½¦ [complete: à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦, POS:NOUN]\\nâ””â”€ à½‚\\n   â””â”€ à½“à½¦ [freq:200, lemma:à½‚à½“à½¦, POS:VERB/NOUN]\\n\\nWHAT MAKES IT \"DICTIONARY-ASSISTED\"?\\n- Combines multiple Tibetan lexicons (Grand Monlam, etc.)\\n- Orthographic normalization (fixes spelling variations)\\n- Layered grammar rules for segmentation and cleaning\\n- Dialect-specific vocabulary support\\n\\nKEY DIFFERENCES FROM PYBO:\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\nâ”‚     Feature        â”‚       PyBo          â”‚       Botok          â”‚\\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\nâ”‚ Lexicon Source     â”‚ THL only            â”‚ Multiple dictionariesâ”‚\\nâ”‚ Tokenization Modes â”‚ One mode            â”‚ Word/Chunk/Space     â”‚\\nâ”‚ Lemmatization      â”‚ No                  â”‚ Yes                  â”‚\\nâ”‚ Affix Handling     â”‚ Basic               â”‚ Configurable         â”‚\\nâ”‚ Custom Dialects    â”‚ No                  â”‚ Yes                  â”‚\\nâ”‚ Spell Normalizationâ”‚ No                  â”‚ Yes                  â”‚\\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n\\nSTRENGTHS:\\nâœ“ More comprehensive dictionary coverage (multiple lexicons)\\nâœ“ Flexible tokenization modes\\nâœ“ Lemmatization support (groups word forms)\\nâœ“ Can handle modern AND classical Tibetan\\nâœ“ Custom dialect support\\nâœ“ Orthographic normalization\\n\\nWEAKNESSES:\\nâœ— Slower than PyBo (more processing steps)\\nâœ— More complex to configure\\nâœ— Still depends on dictionary coverage\\nâœ— May over-normalize spelling in some cases\\n\\nUSE CASE FOR OCR QUALITY SCORING:\\nBotok\\'s multi-dictionary approach might catch MORE valid words than PyBo,\\npotentially giving us better discrimination between good and bad OCR.\\nThe lemmatization feature could also help group variant forms.\\n\\nTOKENIZATION MODES EXPLAINED:\\n1. Word Mode: Segments text into complete words (most common)\\n2. Chunk Mode: Groups \"meaningful character chunks\" (syllables + context)\\n3. Space Mode: Simple space-based splitting (fastest, least accurate)\\n\\nAuthor: Matt\\nDate: 2026-01-25\\nPurpose: Explore Botok\\'s capabilities for potential use in OCR quality scoring\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Botok (à½–à½¼à½‘à¼‹à½à½¼à½‚) - Tibetan Word Tokenizer - Deep Dive Exploration\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "WHAT IS BOTOK?\n",
    "Botok (pronounced [pÊ°Ã¸tÉ”kÌš]) is a state-of-the-art word tokenizer for Tibetan.\n",
    "It's more advanced than PyBo and provides multiple tokenization modes, POS tagging,\n",
    "lemmatization, and support for custom dialects. Developed by OpenPecha.\n",
    "\n",
    "HOW IT WORKS - THE CORE ALGORITHM:\n",
    "1. Dictionary-Based Matching: Uses a Trie data structure built from multiple lexicons\n",
    "2. Hierarchical Rule Templates: Applies layered grammar rules for normalization\n",
    "3. Affix Handling: Can split or keep affixed particles (à½¢à¼‹, à½¦à¼‹, à½ à½²à¼‹, etc.)\n",
    "4. Lemmatization: Reduces words to their base forms\n",
    "5. Multi-Mode: Offers word, chunk, and space-based tokenization\n",
    "\n",
    "THE TRIE DATA STRUCTURE (ENHANCED):\n",
    "Like PyBo, Botok uses a trie, but it's MORE sophisticated:\n",
    "- Supports multiple dictionaries simultaneously\n",
    "- Includes frequency information\n",
    "- Handles morphological variations\n",
    "- Allows custom dialect additions\n",
    "\n",
    "Root\n",
    "â”œâ”€ à½–\n",
    "â”‚  â”œâ”€ à½‘ [freq:100, lemma:à½–à½‘]\n",
    "â”‚  â””â”€ à½‘à½º [freq:500, lemma:à½–à½‘à½º, POS:ADJ]\n",
    "â”‚       â””â”€ à¼‹à½£à½ºà½‚à½¦ [complete: à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦, POS:NOUN]\n",
    "â””â”€ à½‚\n",
    "   â””â”€ à½“à½¦ [freq:200, lemma:à½‚à½“à½¦, POS:VERB/NOUN]\n",
    "\n",
    "WHAT MAKES IT \"DICTIONARY-ASSISTED\"?\n",
    "- Combines multiple Tibetan lexicons (Grand Monlam, etc.)\n",
    "- Orthographic normalization (fixes spelling variations)\n",
    "- Layered grammar rules for segmentation and cleaning\n",
    "- Dialect-specific vocabulary support\n",
    "\n",
    "KEY DIFFERENCES FROM PYBO:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     Feature        â”‚       PyBo          â”‚       Botok          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Lexicon Source     â”‚ THL only            â”‚ Multiple dictionariesâ”‚\n",
    "â”‚ Tokenization Modes â”‚ One mode            â”‚ Word/Chunk/Space     â”‚\n",
    "â”‚ Lemmatization      â”‚ No                  â”‚ Yes                  â”‚\n",
    "â”‚ Affix Handling     â”‚ Basic               â”‚ Configurable         â”‚\n",
    "â”‚ Custom Dialects    â”‚ No                  â”‚ Yes                  â”‚\n",
    "â”‚ Spell Normalizationâ”‚ No                  â”‚ Yes                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "STRENGTHS:\n",
    "âœ“ More comprehensive dictionary coverage (multiple lexicons)\n",
    "âœ“ Flexible tokenization modes\n",
    "âœ“ Lemmatization support (groups word forms)\n",
    "âœ“ Can handle modern AND classical Tibetan\n",
    "âœ“ Custom dialect support\n",
    "âœ“ Orthographic normalization\n",
    "\n",
    "WEAKNESSES:\n",
    "âœ— Slower than PyBo (more processing steps)\n",
    "âœ— More complex to configure\n",
    "âœ— Still depends on dictionary coverage\n",
    "âœ— May over-normalize spelling in some cases\n",
    "\n",
    "USE CASE FOR OCR QUALITY SCORING:\n",
    "Botok's multi-dictionary approach might catch MORE valid words than PyBo,\n",
    "potentially giving us better discrimination between good and bad OCR.\n",
    "The lemmatization feature could also help group variant forms.\n",
    "\n",
    "TOKENIZATION MODES EXPLAINED:\n",
    "1. Word Mode: Segments text into complete words (most common)\n",
    "2. Chunk Mode: Groups \"meaningful character chunks\" (syllables + context)\n",
    "3. Space Mode: Simple space-based splitting (fastest, least accurate)\n",
    "\n",
    "Author: Matt\n",
    "Date: 2026-01-25\n",
    "Purpose: Explore Botok's capabilities for potential use in OCR quality scoring\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf593a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSTALLATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Run this in your terminal (not in the notebook):\n",
    "# pip install botok --break-system-packages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b378bca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b0b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Using file: /home/matt/Documents/tibetan-ocr-app/input_files/toh_4434_T.json\n",
      "ðŸ“š Loaded 3 folios from Tengyur\n",
      "ðŸ“Š Total characters: 4,624\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LOAD TEST DATA FROM TENGYUR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def load_tengyur_sample(json_path: str, num_folios: int = 3) -> List[str]:\n",
    "    \"\"\"Load clean Tibetan text from Tengyur JSON file.\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = [folio['content'] for folio in data[:num_folios]]\n",
    "    \n",
    "    total_chars = sum(len(t) for t in texts)\n",
    "    print(f\"ðŸ“š Loaded {num_folios} folios from Tengyur\")\n",
    "    print(f\"ðŸ“Š Total characters: {total_chars:,}\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Load the test data\n",
    "# Point to the input_files directory in your tibetan-ocr-app project\n",
    "from pathlib import Path\n",
    "input_dir = Path.home() / \"Documents\" / \"tibetan-ocr-app\" / \"input_files\"\n",
    "\n",
    "# Look for JSON files in that directory\n",
    "json_files = list(input_dir.glob(\"*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "    print(f\"âŒ No JSON files found in {input_dir}\")\n",
    "    print(\"   Please add Tengyur JSON files to that directory\")\n",
    "    exit()\n",
    "\n",
    "# Use the first JSON file found\n",
    "tengyur_file = str(json_files[0])\n",
    "print(f\"ðŸ“ Using file: {tengyur_file}\")\n",
    "\n",
    "test_texts = load_tengyur_sample(tengyur_file, num_folios=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e24b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Botok imported successfully\n",
      "\n",
      "======================================================================\n",
      "INITIALIZING BOTOK TOKENIZER\n",
      "======================================================================\n",
      "âœ“ Tokenizer initialized with default settings\n",
      "  Default dictionary: Grand Monlam (plus others)\n",
      "  Mode: Word tokenization\n",
      "  Affixes: Not split by default\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INITIALIZE BOTOK TOKENIZER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "try:\n",
    "    from botok import WordTokenizer\n",
    "    \n",
    "    print(\"âœ… Botok imported successfully\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INITIALIZING BOTOK TOKENIZER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize with default configuration\n",
    "    # This loads the dictionary and builds the Trie\n",
    "    wt = WordTokenizer()\n",
    "    \n",
    "    print(\"âœ“ Tokenizer initialized with default settings\")\n",
    "    print(\"  Default dictionary: Grand Monlam (plus others)\")\n",
    "    print(\"  Mode: Word tokenization\")\n",
    "    print(\"  Affixes: Not split by default\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Botok not installed: {e}\")\n",
    "    print(\"   Install with: pip install botok --break-system-packages\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7277ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: BASIC WORD TOKENIZATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Input text (first 200 chars):\n",
      "   à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢à¼‹à½¦à¾à½‘à¼‹à½‘à½´à¼ à½¦à½²à½‘à¾¡à¾·à¼‹à½¦à¼‹à½¢à¼ à½–à½¼à½‘à¼‹à½¦à¾à½‘à¼‹à½‘à½´à¼ à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼ à½–à½˜à¼‹à½”à½¼à¼‹à½‘à½„à¼‹à½”à½¼à½ à½¼à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½€à¾±à½²à¼‹à½‚à½žà½´à½„à¼‹à½ à½‘à½²à¼‹à½£à¼‹à½£à½ºà½ à½´à¼‹à½–à½¦à¾¡à½´à½¦à¼‹à½”à¼‹à½¦à½´\n",
      "\n",
      "ðŸ” Botok found 40 tokens\n",
      "\n",
      "ðŸ’¡ First 10 tokens:\n",
      "   1. 'à¼„à¼ à¼' (type: PUNCT, POS: )\n",
      "   2. 'à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹' (type: TEXT, POS: NOUN)\n",
      "   3. 'à½‚à½…à½ºà½¦à¼‹à½”à¼‹' (type: TEXT, POS: VERB)\n",
      "   4. 'à½‚à¾²à½´à½–à¼‹à½”à¼‹' (type: TEXT, POS: VERB)\n",
      "   5. 'à½žà½ºà½¦à¼‹' (type: TEXT, POS: PART)\n",
      "   6. 'à½–à¾±à¼‹à½–à¼‹' (type: TEXT, POS: VERB)\n",
      "   7. 'à½£à½ºà½ à½´à¼‹' (type: TEXT, POS: NOUN)\n",
      "   8. 'à½¦à½´à½˜à¼‹à½…à½´à¼‹' (type: TEXT, POS: OTHER)\n",
      "   9. 'à½¢à¾©à¼‹' (type: TEXT, POS: NUM)\n",
      "   10. 'à½‚à½…à½²à½‚à¼‹à½”à¼‹' (type: TEXT, POS: ADJ)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 1: BASIC TOKENIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: BASIC WORD TOKENIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Take a small sample\n",
    "sample_text = test_texts[0][:200]\n",
    "print(f\"\\nðŸ“ Input text (first 200 chars):\")\n",
    "print(f\"   {sample_text}\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = wt.tokenize(sample_text)\n",
    "\n",
    "print(f\"\\nðŸ” Botok found {len(tokens)} tokens\")\n",
    "print(f\"\\nðŸ’¡ First 10 tokens:\")\n",
    "for i, token in enumerate(tokens[:10]):\n",
    "    token_text = getattr(token, 'text', str(token))\n",
    "    token_pos = getattr(token, 'pos', 'N/A')\n",
    "    token_type = getattr(token, 'chunk_type', 'N/A')\n",
    "    print(f\"   {i+1}. '{token_text}' (type: {token_type}, POS: {token_pos})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "383a3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: UNDERSTANDING BOTOK TOKEN ATTRIBUTES\n",
      "======================================================================\n",
      "\n",
      "Botok tokens have rich attributes:\n",
      "Let's examine one token in detail:\n",
      "\n",
      "Token: 'à¼„à¼ à¼'\n",
      "  .text        = 'à¼„à¼ à¼'\n",
      "  .pos         = ''          (part of speech)\n",
      "  .lemma       = ''        (base form)\n",
      "  .chunk_type  = 'PUNCT'   (token category)\n",
      "  .freq        = None           (corpus frequency)\n",
      "\n",
      "ðŸ’¡ Key difference from PyBo: Botok provides LEMMA (base form)\n",
      "   This helps group related word forms together\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 2: UNDERSTANDING TOKEN ATTRIBUTES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: UNDERSTANDING BOTOK TOKEN ATTRIBUTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBotok tokens have rich attributes:\")\n",
    "print(\"Let's examine one token in detail:\\n\")\n",
    "\n",
    "# Find a word token\n",
    "word_tokens = [t for t in tokens if hasattr(t, 'text') and len(t.text) > 1]\n",
    "if word_tokens:\n",
    "    example = word_tokens[0]\n",
    "    \n",
    "    print(f\"Token: '{example.text}'\")\n",
    "    print(f\"  .text        = '{example.text}'\")\n",
    "    \n",
    "    if hasattr(example, 'pos'):\n",
    "        print(f\"  .pos         = '{example.pos}'          (part of speech)\")\n",
    "    \n",
    "    if hasattr(example, 'lemma'):\n",
    "        print(f\"  .lemma       = '{example.lemma}'        (base form)\")\n",
    "    \n",
    "    if hasattr(example, 'chunk_type'):\n",
    "        print(f\"  .chunk_type  = '{example.chunk_type}'   (token category)\")\n",
    "    \n",
    "    if hasattr(example, 'freq'):\n",
    "        print(f\"  .freq        = {example.freq}           (corpus frequency)\")\n",
    "    \n",
    "    if hasattr(example, 'tag'):\n",
    "        print(f\"  .tag         = '{example.tag}'          (grammatical tag)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Key difference from PyBo: Botok provides LEMMA (base form)\")\n",
    "    print(f\"   This helps group related word forms together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc675888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: TOKENIZATION OPTIONS\n",
      "======================================================================\n",
      "\n",
      "Test sentence: à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦à¼\n",
      "\n",
      "1. Default (affixes NOT split):\n",
      "   ['à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹', 'à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦', 'à¼']\n",
      "\n",
      "2. With affixes SPLIT:\n",
      "   ['à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹', 'à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦', 'à¼']\n",
      "\n",
      "ðŸ’¡ Affix splitting can help identify particle errors in OCR\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 3: TOKENIZATION WITH DIFFERENT OPTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: TOKENIZATION OPTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test sentence\n",
    "test_sentence = \"à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦à¼\"\n",
    "\n",
    "print(f\"\\nTest sentence: {test_sentence}\\n\")\n",
    "\n",
    "# Option 1: Default (affixes not split)\n",
    "tokens_default = wt.tokenize(test_sentence, split_affixes=False)\n",
    "print(\"1. Default (affixes NOT split):\")\n",
    "print(f\"   {[t.text for t in tokens_default]}\")\n",
    "\n",
    "# Option 2: Split affixes\n",
    "tokens_split = wt.tokenize(test_sentence, split_affixes=True)\n",
    "print(\"\\n2. With affixes SPLIT:\")\n",
    "print(f\"   {[t.text for t in tokens_split]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Affix splitting can help identify particle errors in OCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c868a701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: LEMMATIZATION (GROUPING WORD FORMS)\n",
      "======================================================================\n",
      "\n",
      "Lemmatization reduces words to their base form:\n",
      "Example: à½–à½€à¾²à¼‹à½¤à½²à½¦ â†’ à½–à½€à¾²à¼‹à½¤à½²à½¦ (base)\n",
      "         à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹à½€à¾±à½² â†’ à½–à½€à¾²à¼‹à½¤à½²à½¦ (base, without genitive particle)\n",
      "\n",
      "Word â†’ Lemma pairs (first 10):\n",
      "   à½žà½ºà½¦à¼‹            â†’ à½…à½ºà½¦à¼‹\n",
      "   à½–à½žà½´à½‚à½¦           â†’ à½–à½žà½´à½‚à½¦à¼‹\n",
      "   à½‘à½´              â†’ à½£à¼‹\n",
      "   à½¦à¼‹à½¢             â†’ à½¦à¼‹à½¢à¼‹\n",
      "   à½‘à½´              â†’ à½£à¼‹\n",
      "   à½žà½ºà½¦à¼‹            â†’ à½…à½ºà½¦à¼‹\n",
      "   à½–à¾±à¼‹à½–            â†’ à½–à¾±à¼‹à½–à¼‹\n",
      "   à½‘à½„à¼‹à½”à½¼à½ à½¼         â†’ à½‘à½„à¼‹à½”à½¼à¼‹\n",
      "   à½€à¾±à½²à¼‹            â†’ à½‚à½²à¼‹\n",
      "   à½¡à½¼à½‘à¼‹\n",
      "           â†’ à½¡à½¼à½‘à¼‹\n",
      "\n",
      "ðŸ’¡ For OCR scoring: Lemmas help group variants, reducing false negatives\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 4: LEMMATIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 4: LEMMATIZATION (GROUPING WORD FORMS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nLemmatization reduces words to their base form:\")\n",
    "print(\"Example: à½–à½€à¾²à¼‹à½¤à½²à½¦ â†’ à½–à½€à¾²à¼‹à½¤à½²à½¦ (base)\")\n",
    "print(\"         à½–à½€à¾²à¼‹à½¤à½²à½¦à¼‹à½€à¾±à½² â†’ à½–à½€à¾²à¼‹à½¤à½²à½¦ (base, without genitive particle)\\n\")\n",
    "\n",
    "# Tokenize with lemmatization enabled\n",
    "sample = test_texts[0][:300]\n",
    "tokens_with_lemma = wt.tokenize(sample, split_affixes=False)\n",
    "\n",
    "# Show words with their lemmas\n",
    "print(\"Word â†’ Lemma pairs (first 10):\")\n",
    "count = 0\n",
    "for token in tokens_with_lemma:\n",
    "    if hasattr(token, 'lemma') and token.lemma and token.text != token.lemma:\n",
    "        print(f\"   {token.text:15s} â†’ {token.lemma}\")\n",
    "        count += 1\n",
    "        if count >= 10:\n",
    "            break\n",
    "\n",
    "if count == 0:\n",
    "    print(\"   (No lemma variations found in this sample)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ For OCR scoring: Lemmas help group variants, reducing false negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75085958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 5: OCR QUALITY SCORING WITH BOTOK\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Scoring known-good Tengyur text:\n",
      "   Total tokens:       287\n",
      "   Valid words:        208\n",
      "   Unknown words:      79\n",
      "   âœ¨ SCORE:           72.47%\n",
      "   â±ï¸  Processing time:  0.2157s\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 5: CALCULATING \"VALID WORD PERCENTAGE\" (OCR QUALITY SCORE)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 5: OCR QUALITY SCORING WITH BOTOK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_botok_score(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate what percentage of text consists of valid Tibetan words.\n",
    "    \n",
    "    Uses Botok's dictionary to identify known vs unknown words.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with scoring metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = wt.tokenize(text, split_affixes=False)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    valid_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Get attributes safely\n",
    "        chunk_type = getattr(token, 'chunk_type', None)\n",
    "        pos = getattr(token, 'pos', None)\n",
    "        \n",
    "        # Skip ONLY punctuation (empty POS)\n",
    "        if chunk_type == 'PUNCT' or pos == '':\n",
    "            continue\n",
    "        \n",
    "        total_tokens += 1\n",
    "        \n",
    "        # A token is \"valid\" if it has a real POS tag\n",
    "        # Check for known invalid markers\n",
    "        if pos and pos not in ['NON_WORD', 'non-word', 'OOV', 'NO_POS', 'OTHER', '', None, 'X']:\n",
    "            valid_tokens += 1\n",
    "        else:\n",
    "            unknown_tokens += 1\n",
    "    \n",
    "    valid_percentage = (valid_tokens / total_tokens * 100) if total_tokens > 0 else 0\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'total_tokens': total_tokens,\n",
    "        'valid_words': valid_tokens,\n",
    "        'unknown_words': unknown_tokens,\n",
    "        'valid_percentage': valid_percentage,\n",
    "        'processing_time': elapsed\n",
    "    }\n",
    "\n",
    "# Test on known-good text\n",
    "good_score = calculate_botok_score(test_texts[0])\n",
    "\n",
    "print(\"\\nðŸ“Š Scoring known-good Tengyur text:\")\n",
    "print(f\"   Total tokens:       {good_score['total_tokens']}\")\n",
    "print(f\"   Valid words:        {good_score['valid_words']}\")\n",
    "print(f\"   Unknown words:      {good_score['unknown_words']}\")\n",
    "print(f\"   âœ¨ SCORE:           {good_score['valid_percentage']:.2f}%\")\n",
    "print(f\"   â±ï¸  Processing time:  {good_score['processing_time']:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9e61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 6: TESTING ON CORRUPTED TEXT (SIMULATED BAD OCR)\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª Testing different OCR quality levels:\n",
      "\n",
      "DIAGNOSTIC CHECK:\n",
      "Original: à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢\n",
      "Corrupted: à½‘à¼à¼à½„à¼‹à¾¨à½“à½‡à½à½”à¾±à½‘à½–à½‚à½…à½–à¼‹à½˜à½„à½‘à½‚à½˜à½–à¼à¼‹à½”à¼‹à½žà¼‹à½¦à½à½‚à¾±à½”à½–à½€à½£à½‡à½‘à½´à¼‹à½…à½€à½à¼‹à½“à½à¼‹à½¢à¾©à¼à½‚à½˜à½²à½‚à½“à½”à¼‹à½–à½žà½´à½‚à½à¼à¼à½€à½…à½“à¼„à¼…à½€à½… à¼à½‡à¾’à¾±à½‘à½‚à½¢\n",
      "\n",
      "First 5 original tokens with POS:\n",
      "  'à¼„à¼ à¼' â†’ POS: \n",
      "  'à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹' â†’ POS: NOUN\n",
      "  'à½‚à½…à½ºà½¦à¼‹à½”à¼‹' â†’ POS: VERB\n",
      "  'à½‚à¾²à½´à½–à¼‹à½”à¼‹' â†’ POS: VERB\n",
      "  'à½žà½ºà½¦à¼‹' â†’ POS: PART\n",
      "\n",
      "First 5 corrupted tokens with POS:\n",
      "  'à½‘' â†’ POS: ADV\n",
      "  'à¼à¼' â†’ POS: \n",
      "  'à½„à¼‹' â†’ POS: PRON\n",
      "  'à¾¨à½“à½‡à½à½”à¾±à½‘à½–à½‚à½…à½–à¼‹' â†’ POS: NON_WORD\n",
      "  'à½˜à½„à½‘à½‚à½˜à½–' â†’ POS: NON_WORD\n",
      "\n",
      "Original (perfect)        â†’ Score:  71.59%\n",
      "20% corrupted             â†’ Score:  32.63%\n",
      "50% corrupted             â†’ Score:  18.89%\n",
      "\n",
      "ðŸ’¡ Botok can also distinguish good OCR from garbage!\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 6: SIMULATING OCR ERRORS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 6: TESTING ON CORRUPTED TEXT (SIMULATED BAD OCR)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def corrupt_text(text: str, corruption_rate: float = 0.2) -> str:\n",
    "    \"\"\"Randomly corrupt text to simulate OCR errors.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    chars = list(text)\n",
    "    num_to_corrupt = int(len(chars) * corruption_rate)\n",
    "    \n",
    "    garbage = ['à½€', 'à½‚', 'à½„', 'à½…', 'à½‡', 'à½', 'à½‘', 'à½“', 'à½”', 'à½–', 'à½˜', 'à¼‹', 'à¼']\n",
    "    \n",
    "    positions = random.sample(range(len(chars)), min(num_to_corrupt, len(chars)))\n",
    "    \n",
    "    for pos in positions:\n",
    "        chars[pos] = random.choice(garbage)\n",
    "    \n",
    "    return ''.join(chars)\n",
    "\n",
    "# Test with different corruption levels\n",
    "sample = test_texts[0][:500]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing different OCR quality levels:\\n\")\n",
    "\n",
    "# DIAGNOSTIC: Show what corruption looks like and how Botok handles it\n",
    "print(\"DIAGNOSTIC CHECK:\")\n",
    "corrupted = corrupt_text(sample, 0.5)\n",
    "print(f\"Original: {sample[:80]}\")\n",
    "print(f\"Corrupted: {corrupted[:80]}\")\n",
    "\n",
    "tokens_orig = wt.tokenize(sample)\n",
    "tokens_corrupt = wt.tokenize(corrupted)\n",
    "\n",
    "print(f\"\\nFirst 5 original tokens with POS:\")\n",
    "for t in tokens_orig[:5]:\n",
    "    print(f\"  '{t.text}' â†’ POS: {getattr(t, 'pos', 'NONE')}\")\n",
    "\n",
    "print(f\"\\nFirst 5 corrupted tokens with POS:\")\n",
    "for t in tokens_corrupt[:5]:\n",
    "    print(f\"  '{t.text}' â†’ POS: {getattr(t, 'pos', 'NONE')}\")\n",
    "print()\n",
    "\n",
    "test_cases = [\n",
    "    (\"Original (perfect)\", sample, 0.0),\n",
    "    (\"20% corrupted\", corrupt_text(sample, 0.2), 0.2),\n",
    "    (\"50% corrupted\", corrupt_text(sample, 0.5), 0.5),\n",
    "]\n",
    "\n",
    "for label, text, rate in test_cases:\n",
    "    score = calculate_botok_score(text)\n",
    "    print(f\"{label:25s} â†’ Score: {score['valid_percentage']:6.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Botok can also distinguish good OCR from garbage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb59507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 7: PERFORMANCE BENCHMARKING\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  How fast is Botok compared to PyBo?\n",
      "   (Important for processing 1,728 outputs per image!)\n",
      "\n",
      "     100 chars â†’  12.38ms (avg of 5 runs)\n",
      "     500 chars â†’ 103.11ms (avg of 5 runs)\n",
      "    1000 chars â†’ 150.32ms (avg of 5 runs)\n",
      "    2000 chars â†’ 223.33ms (avg of 5 runs)\n",
      "\n",
      "ðŸ’¡ For 1,728 outputs:\n",
      "   Estimated scoring time: 385.91s (6.43 minutes)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 7: PERFORMANCE BENCHMARKING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 7: PERFORMANCE BENCHMARKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâ±ï¸  How fast is Botok compared to PyBo?\")\n",
    "print(\"   (Important for processing 1,728 outputs per image!)\\n\")\n",
    "\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "\n",
    "for size in sizes:\n",
    "    text_sample = test_texts[0][:size]\n",
    "    \n",
    "    # Time multiple runs\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.time()\n",
    "        wt.tokenize(text_sample)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    \n",
    "    print(f\"   {size:5d} chars â†’ {avg_time*1000:6.2f}ms (avg of 5 runs)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ For 1,728 outputs:\")\n",
    "estimated_time = 1728 * (sum(times) / len(times))\n",
    "print(f\"   Estimated scoring time: {estimated_time:.2f}s ({estimated_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb28191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 8: TOKENIZATION MODE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Input text: à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢...\n",
      "\n",
      "1. Word mode: 29 tokens\n",
      "   Sample: à¼„à¼_à¼ à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹ à½‚à½…à½ºà½¦à¼‹à½”à¼‹ à½‚à¾²à½´à½–à¼‹à½”à¼‹ à½žà½ºà½¦à¼‹ à½–à¾±à¼‹à½–à¼‹ à½£à½ºà½ à½´à¼‹ à½¦à½´à½˜à¼‹à½…à½´à¼‹ à½¢à¾©à¼‹ à½‚à½…à½²à½‚à¼‹à½”à¼‹ à½–à½žà½´à½‚à½¦ à¼à¼_à¼à¼à¼„à¼…à¼…à¼_à¼ à½¢à¾’à¾±à¼‹à½‚à½¢à¼‹ à½¦à¾à½‘à¼‹ à½‘...\n",
      "\n",
      "2. Chunk mode: 43 tokens\n",
      "   Sample: à¼„à¼_à¼ à½¦à¾¨à½“à¼‹ à½‘à½”à¾±à½‘à¼‹ à½‚à½…à½ºà½¦à¼‹ à½”à¼‹ à½‚à¾²à½´à½–à¼‹ à½”à¼‹ à½žà½ºà½¦à¼‹ à½–à¾±à¼‹ à½–à¼‹ à½£à½ºà½ à½´à¼‹ à½¦à½´à½˜à¼‹ à½…à½´à¼‹ à½¢à¾©à¼‹ à½‚à½…à½²à½‚à¼‹ à½”à¼‹ à½–à½žà½´à½‚à½¦ à¼à¼_à¼à¼à¼„à¼…à¼…à¼_à¼ à½¢à¾’à¾±à¼‹ à½‚à½¢à¼‹...\n",
      "\n",
      "3. Space mode: 8 tokens\n",
      "   Sample: à¼„à¼ à¼à½¦à¾¨à½“à¼‹à½‘à½”à¾±à½‘à¼‹à½‚à½…à½ºà½¦à¼‹à½”à¼‹à½‚à¾²à½´à½–à¼‹à½”à¼‹à½žà½ºà½¦à¼‹à½–à¾±à¼‹à½–à¼‹à½£à½ºà½ à½´à¼‹à½¦à½´à½˜à¼‹à½…à½´à¼‹à½¢à¾©à¼‹à½‚à½…à½²à½‚à¼‹à½”à¼‹à½–à½žà½´à½‚à½¦à¼à¼ à¼à¼à¼„à¼…à¼…à¼ à¼à½¢à¾’à¾±à¼‹à½‚à½¢à¼‹à½¦à¾à½‘à¼‹à½‘à½´à¼ à½¦à½²à½‘à¾¡à¾·à¼‹à½¦à¼‹à½¢à¼ ...\n",
      "\n",
      "ðŸ’¡ For OCR scoring, WORD mode is most useful\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 8: COMPARING CHUNK VS WORD TOKENIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 8: TOKENIZATION MODE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Botok supports multiple tokenization modes\n",
    "# Let's compare them\n",
    "\n",
    "from botok import Text\n",
    "\n",
    "test_snippet = test_texts[0][:150]\n",
    "\n",
    "print(f\"\\nInput text: {test_snippet[:80]}...\\n\")\n",
    "\n",
    "# Create Text object\n",
    "t = Text(test_snippet)\n",
    "\n",
    "# Mode 1: Word tokenization\n",
    "try:\n",
    "    words = t.tokenize_words_raw_text\n",
    "    print(f\"1. Word mode: {len(words.split())} tokens\")\n",
    "    print(f\"   Sample: {words[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"1. Word mode: Error - {e}\")\n",
    "\n",
    "# Mode 2: Chunk tokenization  \n",
    "try:\n",
    "    chunks = t.tokenize_chunks_plaintext\n",
    "    print(f\"\\n2. Chunk mode: {len(chunks.split())} tokens\")\n",
    "    print(f\"   Sample: {chunks[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n2. Chunk mode: Error - {e}\")\n",
    "\n",
    "# Mode 3: Space-based tokenization\n",
    "try:\n",
    "    spaces = t.tokenize_on_spaces\n",
    "    print(f\"\\n3. Space mode: {len(spaces.split())} tokens\")\n",
    "    print(f\"   Sample: {spaces[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n3. Space mode: Error - {e}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ For OCR scoring, WORD mode is most useful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3b9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 9: TESTING EDGE CASES\n",
      "======================================================================\n",
      "\n",
      "ðŸ§ª How does Botok handle unusual inputs?\n",
      "\n",
      "   Mostly punctuation        â†’   0.00% valid\n",
      "   Mixed with numbers        â†’ 100.00% valid\n",
      "   Sanskrit mantra           â†’   0.00% valid\n",
      "   Very short                â†’ 100.00% valid\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXAMPLE 9: EDGE CASES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 9: TESTING EDGE CASES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "edge_cases = {\n",
    "    'Mostly punctuation': 'à¼à¼ à¼à¼à¼ à¼à¼à¼ à¼à¼',\n",
    "    'Mixed with numbers': 'à¼¡à¼¢à¼£ à½–à½¼à½‘à¼‹à½¦à¾à½‘à¼‹ à¼¤à¼¥à¼¦',\n",
    "    'Sanskrit mantra': 'à½¨à½¼à½¾à¼‹à½˜à¼‹à½Žà½²à¼‹à½”à½‘à¾¨à½ºà¼‹à½§à½±à½´à¾ƒà¼‹',\n",
    "    'Very short': 'à½–à½¼à½‘à¼‹',\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ§ª How does Botok handle unusual inputs?\\n\")\n",
    "\n",
    "for label, text in edge_cases.items():\n",
    "    try:\n",
    "        score = calculate_botok_score(text)\n",
    "        print(f\"   {label:25s} â†’ {score['valid_percentage']:6.2f}% valid\")\n",
    "    except Exception as e:\n",
    "        print(f\"   {label:25s} â†’ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09c718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“‹ SUMMARY: BOTOK FOR OCR QUALITY SCORING\n",
      "======================================================================\n",
      "\n",
      "WHAT WE LEARNED:\n",
      "\n",
      "1. Botok is DICTIONARY-ASSISTED - uses multiple comprehensive lexicons\n",
      "2. MULTI-MODE TOKENIZATION - word/chunk/space modes available\n",
      "3. LEMMATIZATION SUPPORT - groups word variants (helpful for OCR)\n",
      "4. RICHER ATTRIBUTES - provides more linguistic information than PyBo\n",
      "5. FLEXIBLE CONFIGURATION - can adapt to different Tibetan dialects\n",
      "\n",
      "STRENGTHS FOR OUR USE CASE:\n",
      "âœ“ Better dictionary coverage (multiple lexicons merged)\n",
      "âœ“ Lemmatization reduces false negatives from OCR variants\n",
      "âœ“ Handles both classical AND modern Tibetan\n",
      "âœ“ Rich token attributes for analysis\n",
      "âœ“ Still reasonably fast for batch processing\n",
      "\n",
      "POTENTIAL WEAKNESSES:\n",
      "âœ— Slightly slower than PyBo (more processing)\n",
      "âœ— More complex configuration options\n",
      "âœ— Still dictionary-dependent (can't handle completely new words)\n",
      "\n",
      "COMPARISON WITH PYBO:\n",
      "- Botok likely has BETTER recall (finds more valid words)\n",
      "- PyBo might be slightly FASTER\n",
      "- Botok provides RICHER linguistic information\n",
      "- Both work well on classical texts\n",
      "\n",
      "RECOMMENDED SCORING FUNCTION:\n",
      "```python\n",
      "def score_ocr_with_botok(text: str) -> float:\n",
      "    tokens = wt.tokenize(text, split_affixes=False)\n",
      "    \n",
      "    valid_tokens = [t for t in tokens \n",
      "                   if hasattr(t, 'pos') and t.pos\n",
      "                   and 'OOV' not in str(t.pos)]\n",
      "    \n",
      "    all_tokens = [t for t in tokens\n",
      "                 if hasattr(t, 'chunk_type')\n",
      "                 and t.chunk_type not in ['PUNCT', 'NON_WORD']]\n",
      "    \n",
      "    return len(valid_tokens) / len(all_tokens) if all_tokens else 0.0\n",
      "```\n",
      "\n",
      "NEXT STEPS:\n",
      "Compare with TibetanRuleSeg to see all three approaches side-by-side!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SUMMARY & RECOMMENDATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‹ SUMMARY: BOTOK FOR OCR QUALITY SCORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT WE LEARNED:\n",
    "\n",
    "1. Botok is DICTIONARY-ASSISTED - uses multiple comprehensive lexicons\n",
    "2. MULTI-MODE TOKENIZATION - word/chunk/space modes available\n",
    "3. LEMMATIZATION SUPPORT - groups word variants (helpful for OCR)\n",
    "4. RICHER ATTRIBUTES - provides more linguistic information than PyBo\n",
    "5. FLEXIBLE CONFIGURATION - can adapt to different Tibetan dialects\n",
    "\n",
    "STRENGTHS FOR OUR USE CASE:\n",
    "âœ“ Better dictionary coverage (multiple lexicons merged)\n",
    "âœ“ Lemmatization reduces false negatives from OCR variants\n",
    "âœ“ Handles both classical AND modern Tibetan\n",
    "âœ“ Rich token attributes for analysis\n",
    "âœ“ Still reasonably fast for batch processing\n",
    "\n",
    "POTENTIAL WEAKNESSES:\n",
    "âœ— Slightly slower than PyBo (more processing)\n",
    "âœ— More complex configuration options\n",
    "âœ— Still dictionary-dependent (can't handle completely new words)\n",
    "\n",
    "COMPARISON WITH PYBO:\n",
    "- Botok likely has BETTER recall (finds more valid words)\n",
    "- PyBo might be slightly FASTER\n",
    "- Botok provides RICHER linguistic information\n",
    "- Both work well on classical texts\n",
    "\n",
    "RECOMMENDED SCORING FUNCTION:\n",
    "```python\n",
    "def score_ocr_with_botok(text: str) -> float:\n",
    "    tokens = wt.tokenize(text, split_affixes=False)\n",
    "    \n",
    "    valid_tokens = [t for t in tokens \n",
    "                   if hasattr(t, 'pos') and t.pos\n",
    "                   and 'OOV' not in str(t.pos)]\n",
    "    \n",
    "    all_tokens = [t for t in tokens\n",
    "                 if hasattr(t, 'chunk_type')\n",
    "                 and t.chunk_type not in ['PUNCT', 'NON_WORD']]\n",
    "    \n",
    "    return len(valid_tokens) / len(all_tokens) if all_tokens else 0.0\n",
    "```\n",
    "\n",
    "NEXT STEPS:\n",
    "Compare with TibetanRuleSeg to see all three approaches side-by-side!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
